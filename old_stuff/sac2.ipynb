{"cells":[{"cell_type":"markdown","metadata":{"id":"RuwAUQ4n0P3B"},"source":["<h1> Soft Actor Critic Demystified</h1>\n","<h4> By Vaishak Kumar </h4>\n","<br>\n","<a href=\"https://arxiv.org/pdf/1801.01290.pdf\">Original Paper</a>\n","<br> \n","<a href=\"https://github.com/higgsfield/RL-Adventure-2\">Adapted from higgsfield's implementation</a>"]},{"cell_type":"code","source":["!apt-get install -y \\\n","    libgl1-mesa-dev \\\n","    libgl1-mesa-glx \\\n","    libglew-dev \\\n","    libosmesa6-dev \\\n","    software-properties-common\n","\n","!apt-get install -y patchelf\n","!pip install free-mujoco-py\n","!pip install box2d-py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GN6VJrkK5Pfb","executionInfo":{"status":"ok","timestamp":1654641464443,"user_tz":300,"elapsed":13870,"user":{"displayName":"Richard Suhendra","userId":"04262243484900749304"}},"outputId":"66fb10f5-9127-4895-d4c8-2c58ae89a338"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","libglew-dev is already the newest version (2.0.0-5).\n","libgl1-mesa-dev is already the newest version (20.0.8-0ubuntu1~18.04.1).\n","libgl1-mesa-glx is already the newest version (20.0.8-0ubuntu1~18.04.1).\n","libosmesa6-dev is already the newest version (20.0.8-0ubuntu1~18.04.1).\n","software-properties-common is already the newest version (0.96.24.32.18).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","patchelf is already the newest version (0.9-1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: free-mujoco-py in /usr/local/lib/python3.7/dist-packages (2.1.6)\n","Requirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (1.15.0)\n","Requirement already satisfied: fasteners==0.15 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (0.15)\n","Requirement already satisfied: Cython<0.30.0,>=0.29.24 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (0.29.30)\n","Requirement already satisfied: imageio<3.0.0,>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (2.19.3)\n","Requirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (1.21.6)\n","Requirement already satisfied: glfw<2.0.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (1.12.0)\n","Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.7/dist-packages (from fasteners==0.15->free-mujoco-py) (1.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fasteners==0.15->free-mujoco-py) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.21)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.7/dist-packages (from imageio<3.0.0,>=2.9.0->free-mujoco-py) (9.1.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VY38JAIX0P3D"},"outputs":[],"source":["import math\n","import random\n","\n","import gym\n","import mujoco_py\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.distributions import Normal\n","\n","from IPython.display import clear_output\n","import matplotlib.pyplot as plt\n","from matplotlib import animation\n","from IPython.display import display\n","\n","%matplotlib inline\n","\n","use_cuda = torch.cuda.is_available()\n","device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"rkRp5Mdd0P3F"},"source":["<h2>Auxilliary Functions</h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vv4e8a3N0P3F"},"outputs":[],"source":["class ReplayBuffer:\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.buffer = []\n","        self.position = 0\n","    \n","    def push(self, state, action, reward, next_state, done):\n","        if len(self.buffer) < self.capacity:\n","            self.buffer.append(None)\n","        self.buffer[self.position] = (state, action, reward, next_state, done)\n","        self.position = (self.position + 1) % self.capacity\n","    \n","    def sample(self, batch_size):\n","        batch = random.sample(self.buffer, batch_size)\n","        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n","        return state, action, reward, next_state, done\n","    \n","    def __len__(self):\n","        return len(self.buffer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"psl3a6040P3G"},"outputs":[],"source":["class NormalizedActions(gym.ActionWrapper):\n","    def action(self, action):\n","        # rescale the action\n","        low, high = self.env.action_space.low, self.env.action_space.high\n","        scaled_action = low + (action + 1.0) * (high - low) / 2.0\n","        scaled_action = np.clip(scaled_action, low, high)\n","        return scaled_action\n","\n","    def reverse_action(self, scaled_action):\n","        low, high = self.env.action_space.low, self.env.action_space.high\n","        action = (scaled_action - low) * 2.0 / (high - low) - 1.0\n","        return action"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MARF2YdZ0P3H"},"outputs":[],"source":["def plot(frame_idx, rewards):\n","    clear_output(True)\n","    plt.figure(figsize=(20,5))\n","    plt.subplot(131)\n","    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n","    plt.plot(rewards)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"t249ym0b0P3I"},"source":["<h1>Network Definitions</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IB6mmawI0P3J"},"outputs":[],"source":["def weights_init_(m):\n","    if isinstance(m, nn.Linear):\n","        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n","        torch.nn.init.constant_(m.bias, 0)\n","\n","class ValueNetwork(nn.Module):\n","    def __init__(self, state_dim, hidden_dim):\n","        super(ValueNetwork, self).__init__()\n","        \n","        self.linear1 = nn.Linear(state_dim, hidden_dim)\n","        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.linear3 = nn.Linear(hidden_dim, 1)\n","        \n","        self.apply(weights_init_)\n","        \n","    def forward(self, state):\n","        x = F.relu(self.linear1(state))\n","        x = F.relu(self.linear2(x))\n","        x = self.linear3(x)\n","        return x\n","        \n","        \n","class DoubleQNetwork(nn.Module):\n","    def __init__(self, num_inputs, num_actions, hidden_size):\n","        super(DoubleQNetwork, self).__init__()\n","        \n","        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size)\n","        self.linear2 = nn.Linear(hidden_size, hidden_size)\n","        self.linear3 = nn.Linear(hidden_size, 1)\n","\n","        self.linear4 = nn.Linear(num_inputs + num_actions, hidden_size)\n","        self.linear5 = nn.Linear(hidden_size, hidden_size)\n","        self.linear6 = nn.Linear(hidden_size, 1)\n","        \n","        self.apply(weights_init_)\n","        \n","    def forward(self, state, action):\n","        xu = torch.cat([state, action], 1)\n","        x1 = F.relu(self.linear1(xu))\n","        x1 = F.relu(self.linear2(x1))\n","        x1 = self.linear3(x1)\n","\n","        x2 = F.relu(self.linear4(xu))\n","        x2 = F.relu(self.linear5(x2))\n","        x2 = self.linear6(x2)\n","        return x1, x2\n","        \n","        \n","class PolicyNetwork(nn.Module):\n","    def __init__(self, num_inputs, num_actions, hidden_size, log_std_min=-20, log_std_max=2):\n","        super(PolicyNetwork, self).__init__()\n","        \n","        self.log_std_min = log_std_min\n","        self.log_std_max = log_std_max\n","        \n","        self.linear1 = nn.Linear(num_inputs, hidden_size)\n","        self.linear2 = nn.Linear(hidden_size, hidden_size)\n","        \n","        self.mean_linear = nn.Linear(hidden_size, num_actions)\n","        self.log_std_linear = nn.Linear(hidden_size, num_actions)\n","        \n","        self.apply(weights_init_)\n","        \n","    def forward(self, state):\n","        x = F.relu(self.linear1(state))\n","        x = F.relu(self.linear2(x))\n","        \n","        mean    = self.mean_linear(x)\n","        log_std = self.log_std_linear(x)\n","        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n","        \n","        return mean, log_std\n","    \n","    def evaluate(self, state, epsilon=1e-6):\n","        mean, log_std = self.forward(state)\n","        std = log_std.exp()\n","        \n","        normal = Normal(0, 1)\n","        z      = normal.sample()\n","        action = torch.tanh(mean+ std*z.to(device))\n","        log_prob = Normal(mean, std).log_prob(mean+ std*z.to(device)) - torch.log(1 - action.pow(2) + epsilon)\n","        log_prob = log_prob.sum(1, keepdim=True)\n","        return action, log_prob\n","        \n","    \n","    def get_action(self, state):\n","        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n","        mean, log_std = self.forward(state)\n","        std = log_std.exp()\n","        \n","        normal = Normal(0, 1)\n","        z      = normal.sample().to(device)\n","        action = torch.tanh(mean + std*z)\n","    \n","        action  = action.cpu()#.detach().cpu().numpy()\n","        return action[0]"]},{"cell_type":"markdown","metadata":{"id":"z-YRnRFJ0P3M"},"source":["<h2> Initializations </h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5526,"status":"ok","timestamp":1654641471393,"user":{"displayName":"Richard Suhendra","userId":"04262243484900749304"},"user_tz":300},"id":"P-712beK0P3M","colab":{"base_uri":"https://localhost:8080/"},"outputId":"97945c81-3bc7-429d-8e81-e6b31b3b1d2c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"]}],"source":["#env = NormalizedActions(gym.make('Walker2d-v2'))\n","env = NormalizedActions(gym.make(\"BipedalWalker-v3\"))\n","#env = NormalizedActions(gym.make('HalfCheetah-v2'))\n","\n","\n","action_dim = env.action_space.shape[0]\n","state_dim  = env.observation_space.shape[0]\n","hidden_dim = 256\n","\n","critic = DoubleQNetwork(state_dim, action_dim, hidden_dim).to(device)\n","critic_target = DoubleQNetwork(state_dim, action_dim, hidden_dim).to(device)\n","policy = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)\n","\n","for target_param, param in zip(critic_target.parameters(), critic.parameters()):\n","    target_param.data.copy_(param.data)\n","\n","entropy_target = -torch.prod(torch.Tensor(env.action_space.shape).to(device)).item()\n","log_alpha = torch.zeros(1, requires_grad=True, device=device)\n","alpha = torch.tensor(0.2)\n","\n","critic_lr = 3e-4\n","policy_lr = 3e-4\n","alpha_lr = 3e-4\n","\n","critic_optimizer = optim.Adam(critic.parameters(), lr=critic_lr)\n","policy_optimizer = optim.Adam(policy.parameters(), lr=policy_lr)\n","alpha_optimizer = optim.Adam([log_alpha], lr=alpha_lr)\n","\n","replay_buffer_size = 1000000\n","replay_buffer = ReplayBuffer(replay_buffer_size)"]},{"cell_type":"markdown","metadata":{"id":"sA3WsvIz0P3K"},"source":["<h1> Update Function </h1>"]},{"cell_type":"markdown","source":["$V\\left(\\mathbf{s}_{t}\\right)=\\mathbb{E}_{\\mathbf{a}_{t} \\sim \\pi}\\left[Q\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)-\\alpha \\log \\pi\\left(\\mathbf{a}_{t} \\mid \\mathbf{s}_{t}\\right)\\right]$\n","\n","$J_{Q}(\\theta)=\\mathbb{E}_{\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right) \\sim \\mathcal{D}}\\left[\\frac{1}{2}\\left(Q_{\\theta}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)-\\left(r\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)+\\gamma \\mathbb{E}_{\\mathbf{s}_{t+1} \\sim p}\\left[V_{\\bar{\\theta}}\\left(\\mathbf{s}_{t+1}\\right)\\right]\\right)\\right)^{2}\\right]$\n","\n","$J_{\\pi}(\\phi)=\\mathbb{E}_{\\mathbf{s}_{t} \\sim \\mathcal{D}, \\epsilon_{t} \\sim \\mathcal{N}}\\left[\\alpha \\log \\pi_{\\phi}\\left(f_{\\phi}\\left(\\epsilon_{t} ; \\mathbf{s}_{t}\\right) \\mid \\mathbf{s}_{t}\\right)-Q_{\\theta}\\left(\\mathbf{s}_{t}, f_{\\phi}\\left(\\epsilon_{t} ; \\mathbf{s}_{t}\\right)\\right)\\right]$\n","\n","$J(\\alpha)=\\mathbb{E}_{\\mathbf{a}_{t} \\sim \\pi_{t}}\\left[-\\alpha \\log \\pi_{t}\\left(\\mathbf{a}_{t} \\mid \\mathbf{s}_{t}\\right)-\\alpha \\overline{\\mathcal{H}}\\right]$"],"metadata":{"id":"h-yYjk_Rt7Fj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6uWAy3r0P3L"},"outputs":[],"source":["def update(batch_size,gamma=0.99,soft_tau=1e-2,):\n","    \n","    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n","\n","    state      = torch.FloatTensor(state).to(device)\n","    next_state = torch.FloatTensor(next_state).to(device)\n","    action     = torch.FloatTensor(action).to(device)\n","    reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)\n","    done       = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n","    \n","# Training Q Function\n","    with torch.no_grad():\n","        new_action, new_log_prob = policy.evaluate(next_state)\n","        qf1_next_target, qf2_next_target = critic_target(next_state, new_action)\n","        min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * new_log_prob\n","        target_q_value = reward + (1-done) * gamma * (min_qf_next_target)\n","    qf1, qf2 = critic(state, action)\n","    qf1_loss = F.mse_loss(qf1, target_q_value.detach())  \n","    qf2_loss = F.mse_loss(qf2, target_q_value.detach())\n","    qf_loss = qf1_loss + qf2_loss\n","\n","    critic_optimizer.zero_grad()\n","    qf_loss.backward()\n","    critic_optimizer.step()\n","\n","# Training Policy Function\n","    pi, log_prob = policy.evaluate(state)\n","    qf1_pi, qf2_pi = critic(state, pi)\n","    min_qf_pi = torch.min(qf1_pi, qf2_pi)\n","    policy_loss = ((alpha * log_prob) - min_qf_pi).mean()\n","\n","    policy_optimizer.zero_grad()\n","    policy_loss.backward()\n","    policy_optimizer.step()\n","\n","# Training Temperature parameter\n","    alpha_loss = -(log_alpha.exp() * (log_prob + entropy_target).detach()).mean()\n","\n","    alpha_optimizer.zero_grad()\n","    alpha_loss.backward()\n","    alpha_optimizer.step()\n","\n","    alpha.data = log_alpha.exp()\n","    \n","    for target_param, param in zip(critic_target.parameters(), critic.parameters()):\n","        target_param.data.copy_(target_param.data * (1.0 - soft_tau) + param.data * soft_tau)"]},{"cell_type":"markdown","metadata":{"id":"qTIMfR_T0P3N"},"source":["# Training Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qW7Kjm-N0P3N"},"outputs":[],"source":["max_frames  = 500000\n","max_steps   = 1000\n","frame_idx   = 0\n","rewards     = []\n","batch_size  = 128\n","explore_before = 5000\n","plot_idx = 2000"]},{"cell_type":"markdown","metadata":{"id":"BQf2Vr5r0P3O"},"source":["# Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":389},"id":"EBUqYvbN0P3O","outputId":"1ed7d7d5-8a20-43d6-b57c-ce516f9ce33e","executionInfo":{"status":"error","timestamp":1654642083711,"user_tz":300,"elapsed":240,"user":{"displayName":"Richard Suhendra","userId":"04262243484900749304"}}},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-8b98dec7b97f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mplot_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-76446c10fcb6>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(batch_size, gamma, soft_tau)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["while frame_idx < max_frames:\n","    state = env.reset()\n","    episode_reward = 0\n","    \n","    for step in range(max_steps):\n","        if frame_idx > explore_before:\n","            action = policy.get_action(state).detach()\n","            next_state, reward, done, _ = env.step(action.numpy())\n","        else:\n","            action = env.action_space.sample()\n","            next_state, reward, done, _ = env.step(action)\n","        \n","        replay_buffer.push(state, action, reward, next_state, done)\n","        \n","        state = next_state\n","        episode_reward += reward\n","        frame_idx += 1\n","        \n","        if len(replay_buffer) > batch_size:\n","            update(batch_size)\n","        \n","        if frame_idx % plot_idx == 0:\n","            plot(frame_idx, rewards)\n","        \n","        if done:\n","            break\n","        \n","    rewards.append(episode_reward)"]},{"cell_type":"code","source":["![title](bipedsac1.png)"],"metadata":{"id":"qZ_U5pZyFo_7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"5vwgi4D5OEAh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"IFMHnMUWOCJM"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"sac2.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}